{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTs4wh-pfCUc",
        "colab_type": "text"
      },
      "source": [
        "#**CA02: Spam Email Detection Using Naive Bayes Classification Algorithm**\n",
        "# Vania Revelina"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFxo5lCYRdTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import necessary packages\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def MakeCounterList(root_dir):\n",
        "  \"\"\"Extracts 3000 most repeated words\n",
        "\n",
        "  Creates a list of every word in every email,\n",
        "  takes only alphabetical words with more than 1 character,\n",
        "  and return 3000 words which occured the most.\n",
        "\n",
        "  Parameter:\n",
        "    root_dir (str): path to a folder which contains the emails\n",
        "\n",
        "  Returns:\n",
        "    word_count_list (list): a list of tuples containing the 3000 most common words\n",
        "                            with its corresponding number of occurences\n",
        "  \"\"\"\n",
        "  all_words = []\n",
        "  # create a list containing all the path to each of the text file (.txt) containing an email\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
        "  for mail in emails: # for every path in the 'emails' list\n",
        "    with open(mail) as m: # open the file at the path selected\n",
        "      for line in m: # for every line in the file\n",
        "        words_list = line.split() # split the line into a list of words\n",
        "        # put these lists of words into 1 list of all the words in the email, we ignore empty lists\n",
        "        # note: the elements of all_words are only words NOT LISTS\n",
        "        all_words += words_list\n",
        "\n",
        "  # counter is a subclass of dictionaries, it counts the number of occurences in iterables\n",
        "  # sample output = Counter({'eggs':2,'milk':3}) meaning the word 'eggs' was repeated 2 times, 'milk' was repeated 3 times\n",
        "  # create a counter subclass containing the number of occurences of each word in all_words list\n",
        "  word_count_list = Counter(all_words)\n",
        "  # when you make a dict to a list, it'll only take the keys, not the values\n",
        "  # set words_to_remove to contain the list of all words in the body of the email.\n",
        "  words_to_remove = all_words\n",
        "\n",
        "  for word in words_to_remove: # for every word in the words_to_remove list\n",
        "    if word.isalpha() == False: # if the word have non-alphabetical characters\n",
        "      del word_count_list[word] # delete the word from the counter dictionary\n",
        "    elif len(word) == 1: # if word ONLY CONTAINS alphabetical characters AND only has 1 character\n",
        "        del word_count_list[word] # delete the word from the counter dictionary\n",
        "    # otherwise, leave it in the counter dictionary\n",
        "  # only take the 3000 words with the most occurences in the counter dictionary\n",
        "  # then turn it into a list of tuples ('word',# of occurence)\n",
        "  word_count_list = word_count_list.most_common(3000)\n",
        "  # return word counter list\n",
        "  return word_count_list\n",
        "\n",
        "\n",
        "def extract_features(mail_dir):\n",
        "  \"\"\"Marks most common word occurence and spam emails\n",
        "\n",
        "  Checks the number of occurences of most common words in each email and marks it in the features_matrix\n",
        "  Checks if the name of the text file containing the email indicates that it is a spam email\n",
        "\n",
        "  Parameter:\n",
        "    mail_dir (str): path to a folder containing all emails\n",
        "\n",
        "  Returns:\n",
        "    features_matrix (np.array): a matrix containing the number of occurences of each most common word in each email\n",
        "    email_labels (np.array): a matrix indicating which emails are spam (1) and not spam (0)\n",
        "\n",
        "  \"\"\"\n",
        "  # create a list of all the path to every email in the folder\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
        "  # create a numpy array of zeros to put the number of occurences of\n",
        "  # the 3000 most common words in each of our email.\n",
        "  features_matrix = np.zeros((len(files),3000))\n",
        "  # create an empty array to put our labels (spam/not spam) in\n",
        "  email_labels = np.zeros(len(files))\n",
        "  # fileNUM indicates the order of the file/email we are in\n",
        "  fileNUM = 0\n",
        "  for file_ in files: # for every path in the files list\n",
        "    with open(file_) as f: # open the file at the path selected\n",
        "      for i, line in enumerate(f):\n",
        "        # i is the order number of the line inside the text file\n",
        "        # example: subject line is line 0 in the text file\n",
        "        # so i=0 for all subject lines\n",
        "        # i=1 is an empty line\n",
        "        # i=2 is the line containing the body of the email\n",
        "        if i == 2: # if it's the body of the email\n",
        "          words = line.split() # create a list of all the words in the body\n",
        "          for word in words: # for every word in the body\n",
        "            wordID = 0\n",
        "            # REMINDER: words_used_dict is a list of tuples of the 3000 most common words\n",
        "            for j, wtup in enumerate(common_words_list): # for j=index and wtup=tuple of (word, number of occurence) in words_used_dict\n",
        "              if wtup[0]==word: # if the most common word selected is the same as the word in the body selected\n",
        "                wordID = j # set wordID to be the index of the most common word\n",
        "                features_matrix[fileNUM,wordID] = words.count(word) # mark the (count of occurences of the most common word in the email body) in the features matrix\n",
        "      filepathTokens = file_.split('/')\n",
        "      # REMINDER: the name of the file that starts with \"spmsg\" indicates that the email is a spam\n",
        "      lastToken = filepathTokens[-1] # get only the name of the file (without the path)\n",
        "      if lastToken.startswith(\"spmsg\"): # if email is a spam\n",
        "        email_labels[fileNUM] = 1 # set label to 1\n",
        "      fileNUM = fileNUM + 1 # go to the next file\n",
        "  return features_matrix, email_labels\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raKYyIrTqF1k",
        "colab_type": "code",
        "outputId": "e317d7b4-f0a8-4eb3-e16d-7fa75f4045e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "\"\"\"The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score.\"\"\"\n",
        "\n",
        "# specify the train and test directory/path\n",
        "TRAIN_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails'\n",
        "TEST_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/test-mails'\n",
        "\n",
        "# create a list of the most common words and the number of occurences\n",
        "common_words_list = MakeCounterList(TRAIN_DIR)\n",
        "print (\"Reading and processing emails from TRAIN and TEST folders\")\n",
        "# create a features matrix and labels for train and test emails sets\n",
        "features_matrix, train_labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\n",
        "\n",
        "# instantiate the Naive Bayes Classification Model\n",
        "model = GaussianNB()\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, train_labels) # train model\n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix) # test model\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels)) # calculate the accuracy of the model\n",
        "\n",
        "\"\"\"======================= END OF PROGRAM =========================\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'======================= END OF PROGRAM ========================='"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40-uiOzDgrPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}